WEBVTT Language: en

00:00:00.000 --> 00:00:03.824
In this chapter I'll introduce the Claude 3 family and what

00:00:03.824 --> 00:00:07.359
it means for enterprise AI. Claude 3 offers multiple model

00:00:07.359 --> 00:00:10.003
sizes targeted at different tradeoffs — intelligence,

00:00:10.003 --> 00:00:13.157
latency, and cost — and all have multimodal vision

00:00:13.157 --> 00:00:14.364
capabilities.

00:00:14.364 --> 00:00:17.531
Compared to Claude 2, these models are faster, more

00:00:17.531 --> 00:00:20.251
steerable, and produce better out-of-the-box results with

00:00:20.251 --> 00:00:21.351
fewer prompt

00:00:21.351 --> 00:00:22.051
tweaks.

00:00:22.051 --> 00:00:24.770
Anthropic positions the largest model as state-of-the-art

00:00:24.770 --> 00:00:27.427
on published benchmarks while smaller variants deliver

00:00:27.427 --> 00:00:30.266
improved speed and lower cost compared to older

00:00:30.266 --> 00:00:31.473
alternatives.

00:00:31.473 --> 00:00:34.680
Finally, Claude 3 is optimized to process long context

00:00:34.680 --> 00:00:37.927
quickly — for example, reading large documents in seconds

00:00:37.927 --> 00:00:40.438
— which unlocks more ambitious workflows in

00:00:40.438 --> 00:00:41.538
production.

00:00:41.538 --> 00:00:44.877
Here we focus on raw performance and Claude 3’s vision

00:00:44.877 --> 00:00:46.088
capabilities.

00:00:46.088 --> 00:00:49.726
The family includes very fast variants — Claude 3 Haiku is

00:00:49.726 --> 00:00:52.402
highlighted for class-leading latency — while larger

00:00:52.402 --> 00:00:55.102
models improve accuracy on hard, open-ended problems,

00:00:55.102 --> 00:00:57.269
sometimes by roughly 2x versus previous

00:00:57.269 --> 00:00:58.469
generations.

00:00:58.469 --> 00:01:01.468
Vision models are trained for enterprise content: charts,

00:01:01.468 --> 00:01:04.286
graphs, technical diagrams, handwritten notes, and images

00:01:04.286 --> 00:01:06.224
from business workflows like insurance

00:01:06.224 --> 00:01:06.924
claims.

00:01:06.924 --> 00:01:09.577
That combination of speed, accuracy, and multimodal

00:01:09.577 --> 00:01:12.781
understanding makes Claude 3 suited to tasks that require

00:01:12.781 --> 00:01:15.710
both rapid processing and reliable outputs across long

00:01:15.710 --> 00:01:17.994
contexts, such as document summarization and

00:01:17.994 --> 00:01:19.215
evidence-based

00:01:19.215 --> 00:01:19.615
Q&A.

00:01:19.615 --> 00:01:22.346
Safety and compliance are core to adopting Claude in

00:01:22.346 --> 00:01:23.346
enterprise

00:01:23.346 --> 00:01:24.246
settings.

00:01:24.246 --> 00:01:26.786
Anthropic emphasizes resistance to jailbreaks — with

00:01:26.786 --> 00:01:29.111
research and internal evaluations showing strong

00:01:29.111 --> 00:01:32.019
performance — and leverages Constitutional AI and RLHF to

00:01:32.019 --> 00:01:34.488
reduce harmful outputs while improving the model’s

00:01:34.488 --> 00:01:35.488
judgments.

00:01:35.488 --> 00:01:38.264
For regulated industries, Claude supports HIPAA and SOC2

00:01:38.264 --> 00:01:41.076
assurances and benefits from AWS Bedrock integrations and

00:01:41.076 --> 00:01:43.834
enterprise security credentials that simplify compliance

00:01:43.834 --> 00:01:45.091
and FedRAMP-adjacent

00:01:45.091 --> 00:01:46.091
workflows.

00:01:46.091 --> 00:01:48.596
Operationally, Anthropic invests in red teaming and

00:01:48.596 --> 00:01:51.443
safety-focused training to reduce false positives and make

00:01:51.443 --> 00:01:53.912
refusals more accurate, enabling use in high-trust

00:01:53.912 --> 00:01:55.353
scenarios like healthcare and

00:01:55.353 --> 00:01:56.153
finance.

00:01:56.153 --> 00:01:58.860
This chapter walks through what teams actually do with

00:01:58.860 --> 00:01:59.560
Claude.

00:01:59.560 --> 00:02:02.208
Common applications include content generation (copy,

00:02:02.208 --> 00:02:05.076
documentation, email drafts), long-form summarization and

00:02:05.076 --> 00:02:07.503
Q&A across books, contracts, and transcripts, and

00:02:07.503 --> 00:02:10.371
sophisticated classification and extraction of structured

00:02:10.371 --> 00:02:10.871
data.

00:02:10.871 --> 00:02:13.773
In developer workflows Claude supports code generation, SQL

00:02:13.773 --> 00:02:15.341
synthesis, unit tests, and code

00:02:15.341 --> 00:02:16.541
explanation.

00:02:16.541 --> 00:02:19.321
It also works as a dialogue agent for customer support,

00:02:19.321 --> 00:02:20.593
tutoring, or internal

00:02:20.593 --> 00:02:21.493
advisors.

00:02:21.493 --> 00:02:23.993
Two important enablers across use cases are native

00:02:23.993 --> 00:02:26.861
multilingual handling (200+ languages) and RAG integration

00:02:26.861 --> 00:02:29.702
to ground responses in customer data for higher precision

00:02:29.702 --> 00:02:30.702
and reduced

00:02:30.702 --> 00:02:31.923
hallucination.

00:02:31.923 --> 00:02:34.777
Prompt engineering is an empirical craft: prompts contain

00:02:34.777 --> 00:02:37.236
context, tone, background data, clear task rules,

00:02:37.236 --> 00:02:38.592
examples, and conversation

00:02:38.592 --> 00:02:39.392
history.

00:02:39.392 --> 00:02:42.218
Good prompts are tested against comprehensive testcases,

00:02:42.218 --> 00:02:45.021
including edge cases like poor input, ambiguous queries,

00:02:45.021 --> 00:02:45.921
or missing

00:02:45.921 --> 00:02:46.421
data.

00:02:46.421 --> 00:02:49.017
Consumer prompts tend to be shorter and exploratory;

00:02:49.017 --> 00:02:51.447
enterprise prompts are templatized with variable

00:02:51.447 --> 00:02:53.142
placeholders for high-throughput

00:02:53.142 --> 00:02:54.242
automation.

00:02:54.242 --> 00:02:56.940
With Claude 3 you must use the Messages API and respect

00:02:56.940 --> 00:02:59.539
user/assistant/system role formatting — separating

00:02:59.539 --> 00:03:02.195
system-level instructions from user content improves

00:03:02.195 --> 00:03:03.833
steerability and reduces leakage

00:03:03.833 --> 00:03:04.433
risks.

00:03:04.433 --> 00:03:07.391
The guiding principle: iterate, measure, and refine prompts

00:03:07.391 --> 00:03:08.630
against held-out

00:03:08.630 --> 00:03:09.230
evals.

00:03:09.230 --> 00:03:12.095
This section covers practical techniques that materially

00:03:12.095 --> 00:03:13.295
improve model

00:03:13.295 --> 00:03:14.095
outputs.

00:03:14.095 --> 00:03:17.486
Be explicit and direct, number steps for complex tasks, and

00:03:17.486 --> 00:03:19.362
assign roles to shift tone and

00:03:19.362 --> 00:03:20.262
accuracy.

00:03:20.262 --> 00:03:22.732
XML tags and template variables make prompts

00:03:22.732 --> 00:03:24.279
machine-friendly and minimize

00:03:24.279 --> 00:03:25.279
ambiguity;

00:03:25.279 --> 00:03:28.374
pre-filling the assistant field steers format and increases

00:03:28.374 --> 00:03:29.274
accuracy.

00:03:29.274 --> 00:03:32.454
Asking Claude to think step-by-step or to output internal

00:03:32.454 --> 00:03:35.319
reasoning (in tagged blocks) often improves correctness,

00:03:35.319 --> 00:03:38.107
especially for multi-step logic, while n-shot examples

00:03:38.107 --> 00:03:39.982
reduce variance at the cost of

00:03:39.982 --> 00:03:40.682
tokens.

00:03:40.682 --> 00:03:43.894
For complex workflows use chaining — have Claude generate,

00:03:43.894 --> 00:03:47.190
then evaluate or rewrite against rubrics — and feed long

00:03:47.190 --> 00:03:49.384
documents in tagged blocks before asking

00:03:49.384 --> 00:03:50.384
questions.

00:03:50.384 --> 00:03:53.520
Tools and retrieval make Claude practical for real systems.

00:03:53.520 --> 00:03:56.539
Tool use means Claude decides which function to call and

00:03:56.539 --> 00:03:57.339
with what

00:03:57.339 --> 00:03:58.339
arguments;

00:03:58.339 --> 00:04:01.330
the client executes the function and returns results for

00:04:01.330 --> 00:04:02.130
Claude to

00:04:02.130 --> 00:04:02.930
consume.

00:04:02.930 --> 00:04:05.970
Describe functions and call syntax clearly in prompts and

00:04:05.970 --> 00:04:08.865
use XML tags around <function_calls> so outputs can be

00:04:08.865 --> 00:04:09.565
parsed.

00:04:09.565 --> 00:04:12.626
RAG complements this by embedding and retrieving documents

00:04:12.626 --> 00:04:14.915
from vector DBs so answers are grounded in

00:04:14.915 --> 00:04:15.815
evidence;

00:04:15.815 --> 00:04:18.780
good RAG systems include chunking strategy, reranking or

00:04:18.780 --> 00:04:21.453
Claude-driven query-rewrites, and selection of the

00:04:21.453 --> 00:04:22.553
appropriate

00:04:22.553 --> 00:04:23.453
database.

00:04:23.453 --> 00:04:26.514
Together, tooling and RAG reduce hallucinations and enable

00:04:26.514 --> 00:04:27.846
accurate, actionable

00:04:27.846 --> 00:04:28.846
responses.

00:04:28.846 --> 00:04:32.378
We wrap up with empirical evaluations, migration notes, and

00:04:32.378 --> 00:04:33.078
API best

00:04:33.078 --> 00:04:34.078
practices.

00:04:34.078 --> 00:04:37.924
Evals are the backbone of prompt engineering — use MCQs for

00:04:37.924 --> 00:04:41.158
closed form checks, exact match for string-level tasks,

00:04:41.158 --> 00:04:43.214
and open-answer rubrics for nuanced

00:04:43.214 --> 00:04:44.214
judgments.

00:04:44.214 --> 00:04:47.797
Run held-out test suites and include edge cases to avoid

00:04:47.797 --> 00:04:48.997
regressions.

00:04:48.997 --> 00:04:52.667
Migrating from Claude 2 to 3 often means moving to the

00:04:52.667 --> 00:04:55.779
Messages API, leveraging assistant prefill, and trimming

00:04:55.779 --> 00:04:58.518
prompts because Claude 3 can do more with

00:04:58.518 --> 00:04:59.018
less.

00:04:59.018 --> 00:05:02.217
Finally, tune max_tokens to guard response length, use

00:05:02.217 --> 00:05:05.450
stop_sequences (paired with XML tags) to limit outputs,

00:05:05.450 --> 00:05:09.069
and set temperature near 0 for analytical tasks or closer

00:05:09.069 --> 00:05:09.669
to 1 for

00:05:09.669 --> 00:05:10.769
creativity.
